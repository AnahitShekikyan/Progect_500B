{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNH59A933+3ty1wWIdfM2+L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnahitShekikyan/Progect_500B/blob/main/ADS_502_Final_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkluQw4Hjl1W"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "cancer = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#library imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "NMGwInzwj_Wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import data\n",
        "df_original = pd.read_csv(\"/content/breast-cancer.csv\")\n",
        "df_original.head()\n"
      ],
      "metadata": {
        "id": "csoHKuomkC4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data trim, per study \"best predictive accuracy obtained using one separating plane in the 3-D space of Worst Area, Worst Smoothness and Mean Texture.\"\n",
        "df = df_original[['diagnosis', 'area_worst', 'smoothness_worst', 'texture_mean']]\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "IR8sQcGwlKlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert diagnosis to binary\n",
        "df.loc[:, 'diagnosis'] = df['diagnosis'].replace({'M': 1, 'B': 0})\n",
        "df.head()"
      ],
      "metadata": {
        "id": "cq4E9WW9lUSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Basic Data Information**"
      ],
      "metadata": {
        "id": "rSn-_bhXlc9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get shape\n",
        "df.shape"
      ],
      "metadata": {
        "id": "B7c3biyZlbcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#are there duplicates?\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "CNqt-JlEliNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['diagnosis'] = df['diagnosis'].astype(int)"
      ],
      "metadata": {
        "id": "UVBNgjFjltU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='diagnosis', data=df)"
      ],
      "metadata": {
        "id": "dxznpP-bluGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#percentage of binary class\n",
        "print(\"percentage of each class\", df['diagnosis'].value_counts()/len(df)*100)"
      ],
      "metadata": {
        "id": "TGCf-iO-lyWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Quality Report**\n",
        "\n",
        "## **Continuous Features**"
      ],
      "metadata": {
        "id": "K7EA-ZHtl46S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# identify continuous features\n",
        "conf = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "conf"
      ],
      "metadata": {
        "id": "BsUtdAU4l1Ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#identify any columns to filter out from the \"continuous features\"\n",
        "conf_exclude = ['']\n",
        "filter_conf = [x for x in conf if x not in conf_exclude]\n",
        "filter_conf"
      ],
      "metadata": {
        "id": "Zk-1zvOzl_Bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get summary stats on continuous\n",
        "df[filter_conf].describe()"
      ],
      "metadata": {
        "id": "pYCkHqxFmDsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_quality_conf = pd.DataFrame({\n",
        "    'Feature': filter_conf,\n",
        "    'Count': df[filter_conf].count().values,\n",
        "    'Missing Values': df[filter_conf].isnull().sum().values,\n",
        "    'Cardinality': df[filter_conf].nunique().values,\n",
        "    'Min': df[filter_conf].min().values,\n",
        "    '1st Quartile': df[filter_conf].quantile(0.25).values,\n",
        "    'Mean': df[filter_conf].mean().values,\n",
        "    'Median': df[filter_conf].median().values,\n",
        "    '3rd Quartile': df[filter_conf].quantile(0.75).values,\n",
        "    'Max': df[filter_conf].max().values,\n",
        "    'Standard Deviation': df[filter_conf].std().values,\n",
        "})\n",
        "data_quality_conf"
      ],
      "metadata": {
        "id": "5KX5j5lWmo1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Univariate Analysis**"
      ],
      "metadata": {
        "id": "PDlU6ZKBmtD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plot histograms for numerical variables\n",
        "plt.style.use('ggplot')\n",
        "for column in filter_conf:\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.histplot(df[column], kde = True)\n",
        "    plt.title(f'Distribution of {column}')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "yPTc6FeDmrfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot boxplots of all continuous features\n",
        "plt.style.use('ggplot')\n",
        "for column in filter_conf:\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.boxplot(x=df[column])\n",
        "    plt.title(f'Boxplot of {column}')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "tc3Z4K5Em0Kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multivariate Analysis**"
      ],
      "metadata": {
        "id": "7p3O-cwhm4iU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#correlations, all\n",
        "corr_matrix = df[filter_conf].corr()\n",
        "corr_matrix"
      ],
      "metadata": {
        "id": "_vxz6Gipm3Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a heatmap\n",
        "plt.figure(figsize=(16, 12))\n",
        "heatmap = sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', linewidths=0.5, annot_kws={\"size\": 8})\n",
        "\n",
        "# Rotate the x and y labels for better readability\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "\n",
        "# Show the heatmap\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ai-38IZlnAut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(df, hue =\"diagnosis\", height=3)"
      ],
      "metadata": {
        "id": "aUHgAqJBnC5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Scaling**\n",
        "\n",
        "[link text](https://)"
      ],
      "metadata": {
        "id": "ViiwWv3nnHBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Isolate features (X) and target (y)\n",
        "X = df[['area_worst', 'smoothness_worst', 'texture_mean']]\n",
        "y = df['diagnosis']\n",
        "\n",
        "# Scaling features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "JufeEi3CnFLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stratified K-Fold Partitioning**"
      ],
      "metadata": {
        "id": "uQfawvBAnT_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# Initialize StratifiedKFold with 5 folds\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# get list of partitions\n",
        "def get_partitions(X, y):\n",
        "  partitions = []\n",
        "  # Performing stratified k-fold cross-validation\n",
        "  for train_index, test_index in skf.split(X, y):\n",
        "      X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
        "      y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "      partitions.append((X_train, X_test, y_train, y_test))\n",
        "  return partitions"
      ],
      "metadata": {
        "id": "vgUNy1bdnS7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visualizations**"
      ],
      "metadata": {
        "id": "s2s9oICSnZXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define color maps for visualization\n",
        "cmap_cv = plt.get_cmap('coolwarm')\n",
        "cmap_data = plt.get_cmap('tab10')\n",
        "\n",
        "# Define visualization function for cross-validation indices\n",
        "def plot_cv_indices(cv, X, y, ax, n_splits, lw=10):\n",
        "\n",
        "    #Create a plot for indices of a cross-validation object\n",
        "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y)):\n",
        "        indices = np.array([np.nan] * len(X))\n",
        "        indices[tt] = 1  # Testing set\n",
        "        indices[tr] = 0  # Training set\n",
        "        ax.scatter(range(len(indices)), [ii + 0.5] * len(indices), c=indices, marker=\"_\", lw=lw, cmap=cmap_cv, vmin=-0.2, vmax=1.2)\n",
        "\n",
        "    ax.scatter(range(len(X)), [ii + 1.5] * len(X), c=y, marker=\"_\", lw=lw, cmap=cmap_data)\n",
        "    yticklabels = list(range(n_splits)) + [\"class\"]\n",
        "    ax.set(yticks=np.arange(n_splits + 1) + 0.5, yticklabels=yticklabels, xlabel=\"Sample index\", ylabel=\"CV iteration\", ylim=[n_splits + 1.2, -0.2], xlim=[0, len(X)])\n",
        "    ax.set_title(\"Cross-Validation Splits\", fontsize=15)\n",
        "    return ax\n",
        "\n",
        "# Creating a plot\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "plot_cv_indices(skf, X_scaled, y, ax, n_splits=5)\n",
        "plt.show()\n",
        "\n",
        "# Plotting fold distribution\n",
        "def plot_fold_distribution(cv, X, y, ax):\n",
        "    fold_sizes = [np.sum(y.iloc[tt] == 1) for _, tt in cv.split(X, y)]\n",
        "    class_0 = [np.sum(y.iloc[tt] == 0) for _, tt in cv.split(X, y)]\n",
        "    class_1 = [np.sum(y.iloc[tt] == 1) for _, tt in cv.split(X, y)]\n",
        "\n",
        "    df_fold = pd.DataFrame({'Fold': list(range(len(fold_sizes))), 'Class 0': class_0, 'Class 1': class_1})\n",
        "    df_fold.set_index('Fold').plot(kind='bar', ax=ax)\n",
        "    ax.set_xlabel('Fold')\n",
        "    ax.set_ylabel('Count')\n",
        "    ax.set_title('Distribution of Classes Across Folds')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "plot_fold_distribution(skf, X_scaled, y, ax)\n",
        "plt.show()\n",
        "\n",
        "# Plotting class distribution heatmap\n",
        "def plot_class_distribution_heatmap(cv, X, y, ax):\n",
        "    fold_class_distribution = []\n",
        "    for train_idx, test_idx in cv.split(X, y):\n",
        "        fold_class_distribution.append(np.bincount(y.iloc[test_idx], minlength=2))\n",
        "\n",
        "    df_class_dist = pd.DataFrame(fold_class_distribution, columns=['Class 0', 'Class 1'])\n",
        "    sns.heatmap(df_class_dist, annot=True, cmap='Blues', fmt='d', ax=ax)\n",
        "    ax.set_xlabel('Class')\n",
        "    ax.set_ylabel('Fold')\n",
        "    ax.set_title('Class Distribution Across Folds')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "plot_class_distribution_heatmap(skf, X_scaled, y, ax)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ErYDIEIVnY5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-Validation Splits Plot\n",
        "\n",
        "Description:\n",
        "\n",
        "*   The first plot shows the indices of samples used for training (blue) and testing (red) across different cross-validation (CV) iterations.\n",
        "\n",
        "*   The \"class\" row at the bottom indicates the actual class distribution of the samples.\n",
        "\n",
        "Key Points:\n",
        "\n",
        "*   **Consistent Distribution:** The plot shows a consistent distribution of training and testing samples across all folds, which ensures that each fold has a representative sample of the overall dataset.\n",
        "\n",
        "*   **Class Distribution:** The \"class\" row indicates the balance of the classes in the dataset, which appears to be relatively balanced.\n",
        "\n",
        "\n",
        "Usage:\n",
        "\n",
        "*   This plot is useful for visualizing how the data is split into training and testing sets during each fold of the cross-validation process.\n",
        "\n",
        "*   It helps in ensuring that the stratification process is working correctly, preserving the class distribution in each fold.\n",
        "\n",
        "## Distribution of Classes Across Folds\n",
        "\n",
        "Description:\n",
        "\n",
        "*   The second plot is a bar chart showing the distribution of classes (Class 0 and Class 1) across different folds.\n",
        "\n",
        "*   The x-axis represents the fold number, and the y-axis represents the count of samples for each class.\n",
        "\n",
        "\n",
        "Key Points:\n",
        "\n",
        "*   **Balanced Classes:** Each fold contains a similar number of samples for Class 0 and Class 1, indicating that the stratified cross-validation is preserving the class balance.\n",
        "\n",
        "*   **Uniform Distribution:** The consistent height of the bars across folds shows that the distribution of classes is uniform across all folds.\n",
        "\n",
        "Usage:\n",
        "\n",
        "*   This plot is useful for confirming that each fold has a balanced distribution of the target classes.\n",
        "\n",
        "*   It helps in validating the effectiveness of the StratifiedKFold method in maintaining class balance.\n",
        "\n",
        "## Class Distribution Heatmap\n",
        "\n",
        "Description:\n",
        "\n",
        "*   The third plot is a heatmap showing the distribution of classes across different folds.\n",
        "\n",
        "*   The x-axis represents the classes (Class 0 and Class 1), and the y-axis represents the fold number.\n",
        "\n",
        "\n",
        "Key Points:\n",
        "\n",
        "*  **Counts per Class:** The heatmap values indicate the count of each class in each fold.\n",
        "\n",
        "*   **Color Intensity:** The color intensity represents the number of samples, with darker colors indicating higher counts.\n",
        "\n",
        "Usage:\n",
        "\n",
        "*   This plot provides a visual representation of the class distribution across folds.\n",
        "\n",
        "*   It is useful for quickly identifying any discrepancies in the distribution of classes in different folds.\n",
        "\n",
        "**Summary**\n",
        "\n",
        "**Cross-Validation Splits Plot:** Ensures proper stratification and visualizes the data splitting process.\n",
        "\n",
        "**Distribution of Classes Across Folds:** Confirms the uniform distribution of classes across folds, validating the effectiveness of stratified cross-validation.\n",
        "\n",
        "**Class Distribution Heatmap:** Provides a detailed view of class counts in each fold, highlighting the balance maintained across folds.\n"
      ],
      "metadata": {
        "id": "Brgs2iCtnnFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "s5pHLzQ8nzra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline Model (Logistic Regression)"
      ],
      "metadata": {
        "id": "hkoZjnmyn1-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and print mean and standard deviation of each metric\n",
        "def print_metrics_summary(name, metrics):\n",
        "    mean_metric = np.mean(metrics)\n",
        "    std_metric = np.std(metrics, ddof=1)\n",
        "    print(f'Mean {name}: {mean_metric:.4f}')\n",
        "    print(f'Standard Deviation of {name}: {std_metric:.4f}')"
      ],
      "metadata": {
        "id": "hLElx3oDnimO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_metrics = {\n",
        "    \"logistic_regression\": {},\n",
        "    \"neural_net\": {},\n",
        "    \"random_forest\": {}\n",
        "}"
      ],
      "metadata": {
        "id": "ZIRA6Wbgn7FW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists to store metrics\n",
        "accuracy_scores = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "roc_aucs = []\n",
        "\n",
        "for part in get_partitions(X_scaled, y):\n",
        "    # Unpack partition into constituent variables\n",
        "    (X_train, X_test, y_train, y_test) = part\n",
        "\n",
        "    # Initializing the logistic regression model\n",
        "    model = LogisticRegression(max_iter=10000)\n",
        "\n",
        "    # Train the model on the training data\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics for this fold\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    y_prob = model.predict_proba(X_test)[:, 1]\n",
        "    roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "    # Append metrics to lists\n",
        "    accuracy_scores.append(accuracy)\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1_scores.append(f1)\n",
        "    roc_aucs.append(roc_auc)\n",
        "\n",
        "    # Print metrics for this fold\n",
        "    print(f'Fold Accuracy: {accuracy:.4f}')\n",
        "    print(f'Fold Precision: {precision:.4f}')\n",
        "    print(f'Fold Recall: {recall:.4f}')\n",
        "    print(f'Fold F1 Score: {f1:.4f}')\n",
        "    print(f'Fold ROC AUC: {roc_auc:.4f}')\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# display aggregate eval metrics\n",
        "print_metrics_summary('Accuracy', accuracy_scores)\n",
        "print_metrics_summary('Precision', precisions)\n",
        "print_metrics_summary('Recall', recalls)\n",
        "print_metrics_summary('F1 Score', f1_scores)\n",
        "print_metrics_summary('ROC AUC', roc_aucs)"
      ],
      "metadata": {
        "id": "5xLPj2AGn8-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output metrics indicate that the model is performing quite well, here is summary:\n",
        "\n",
        "**Fold-wise Metrics**\n",
        "\n",
        "\n",
        "1.   Fold Accuracy:\n",
        "\n",
        "    *   Fold 1: 0.9649\n",
        "\n",
        "    *   Fold 2: 0.9474\n",
        "\n",
        "    *   Fold 3: 0.9649\n",
        "\n",
        "    *   Fold 4: 0.9825\n",
        "\n",
        "    *   Fold 5: 0.9558\n",
        "\n",
        "\n",
        "2.   Fold Precision:\n",
        "\n",
        "    *   Fold 1:  0.9535\n",
        "\n",
        "    *   Fold 2: 1.0000\n",
        "\n",
        "    *   Fold 3: 1.0000\n",
        "\n",
        "    *   Fold 4: 0.9545\n",
        "\n",
        "    *   Fold 5: 0.9744\n",
        "\n",
        "\n",
        "3.   Fold Recall:\n",
        "\n",
        "    *   Fold 1: 0.9535\n",
        "\n",
        "    *   Fold 2: 0.8605\n",
        "\n",
        "    *   Fold 3: 0.9048\n",
        "\n",
        "    *   Fold 4: 1.0000\n",
        "\n",
        "    *   Fold 5: 0.9048\n",
        "\n",
        "   \n",
        "4.   Fold F1 Score:\n",
        "\n",
        "    *   Fold 1: 0.9535\n",
        "\n",
        "    *   Fold 2: 0.9250\n",
        "\n",
        "    *   Fold 3: 0.9500\n",
        "\n",
        "    *   Fold 4: 0.9767\n",
        "\n",
        "    *   Fold 5: 0.9383\n",
        "\n",
        "\n",
        " 5.   Fold ROC AUC:\n",
        "\n",
        "    *   Fold 1: 0.9980\n",
        "\n",
        "    *   Fold 2: 0.9866\n",
        "\n",
        "    *   Fold 3: 0.9818\n",
        "\n",
        "    *   Fold 4: 1.0000\n",
        "\n",
        "    *   Fold 5: 0.9950\n",
        "\n",
        "\n",
        "**Summary Metrics**\n",
        "\n",
        "*   Mean Accuracy: 0.9631\n",
        "\n",
        "*   Standard Deviation of Accuracy: 0.0131\n",
        "\n",
        "*   Mean Precision: 0.9765\n",
        "\n",
        "*   Standard Deviation of Precision: 0.0230\n",
        "\n",
        "*   Mean Recall: 0.9247\n",
        "\n",
        "*   Standard Deviation of Recall: 0.0534\n",
        "\n",
        "*   Mean F1 Score: 0.9487\n",
        "\n",
        "*   Standard Deviation of F1 Score: 0.0192\n",
        "\n",
        "*   Mean ROC AUC: 0.9923\n",
        "\n",
        "*   Standard Deviation of ROC AUC: 0.0078\n",
        "\n",
        "\n",
        "**Accuracy:** The accuracy is consistently high across all folds, indicating that the model is performing well in distinguishing classes.\n",
        "\n",
        "\n",
        "**Precision and Recall:** High precision indicates few false positives, while recall values are slightly more variable, reflecting differences in detecting true positives across folds.\n",
        "\n",
        "**F1 Score:** This combines precision and recall, showing balanced performance.\n",
        "\n",
        "\n",
        "**ROC AUC:** High values close to 1 indicate excellent performance in distinguishing classes.\n",
        "\n",
        "**Confusion Matrices**\n",
        "\n",
        "*   Fold 1: [[69 2], [ 2 41]]\n",
        "\n",
        "*   Fold 2: [[71 0], [ 6 37]]\n",
        "\n",
        "*   Fold 3: [[72 0], [ 4 38]]\n",
        "\n",
        "*   Fold 4: [[70 2], [ 0 42]]\n",
        "\n",
        "*   Fold 5: [[70 1], [ 4 38]]\n",
        "\n",
        "\n",
        "\n",
        "**Conclusion**\n",
        "The standard deviations are relatively low, suggesting stable performance. The use of StratifiedKFold ensures that the class distribution is maintained across each fold, contributing to reliable validation."
      ],
      "metadata": {
        "id": "LpMm6zhFoFKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network"
      ],
      "metadata": {
        "id": "cbnbcbfPoUyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Initialize lists to store metrics\n",
        "accuracy_scores = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "roc_aucs = []\n",
        "\n",
        "for part in get_partitions(X_scaled, y):\n",
        "    # Unpack partition into constituent variables\n",
        "    (X_train, X_test, y_train, y_test) = part\n",
        "\n",
        "    clf = MLPClassifier(solver='lbfgs', max_iter=100, alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=2)\n",
        "\n",
        "    # train the model\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics for this fold\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    y_prob = model.predict_proba(X_test)[:, 1]\n",
        "    roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "    # Append metrics to lists\n",
        "    accuracy_scores.append(accuracy)\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1_scores.append(f1)\n",
        "    roc_aucs.append(roc_auc)\n",
        "\n",
        "    # Print metrics for this fold\n",
        "    print(f'Fold Accuracy: {accuracy:.4f}')\n",
        "    print(f'Fold Precision: {precision:.4f}')\n",
        "    print(f'Fold Recall: {recall:.4f}')\n",
        "    print(f'Fold F1 Score: {f1:.4f}')\n",
        "    print(f'Fold ROC AUC: {roc_auc:.4f}')\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# display aggregate eval metrics\n",
        "print_metrics_summary('Accuracy', accuracy_scores)\n",
        "print_metrics_summary('Precision', precisions)\n",
        "print_metrics_summary('Recall', recalls)\n",
        "print_metrics_summary('F1 Score', f1_scores)\n",
        "print_metrics_summary('ROC AUC', roc_aucs)"
      ],
      "metadata": {
        "id": "rFleLqmToBSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Comparison\n",
        "\n",
        "## ROC Curve Plotting"
      ],
      "metadata": {
        "id": "CdtzeiwzofDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot ROC curve for each fold\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "mean_tpr = np.zeros_like(mean_fpr)\n",
        "\n",
        "for train_index, test_index in skf.split(X_scaled, y):\n",
        "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_prob = model.predict_proba(X_test)[:, 1]\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "    mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, lw=1, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "\n",
        "mean_tpr /= skf.get_n_splits()\n",
        "mean_auc = auc(mean_fpr, mean_tpr)\n",
        "\n",
        "# Plot settings\n",
        "plt.plot(mean_fpr, mean_tpr, color='b', lw=2, label=f'Mean ROC curve (area = {mean_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LfDQiUsSoaHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Roc curve  plots the True Positive Rate (TPR) against the False Positive Rate (FPR).\n",
        "\n",
        "**Mean ROC Curve:**\n",
        "\n",
        "*   The mean ROC curve, plotted in blue, represents the average performance across all folds.\n",
        "\n",
        "*   It has an AUC of 0.99, indicating excellent overall performance.\n",
        "\n",
        "**Diagonal Line:** The dashed diagonal line represents the ROC curve of a random classifier with an AUC of 0.50. This serves as a baseline for comparison.\n",
        "\n",
        "**AUC**: The AUC values for the individual folds are very high (ranging from 0.98 to 1.00), indicating that the model performs exceptionally well in distinguishing between the positive and negative classes.\n",
        "\n",
        "The ROC curves for different folds are very close to each other, demonstrating consistent performance across the cross-validation folds. This suggests that the model's performance is stable and not heavily influenced by the particular train-test split.\n",
        "\n",
        "The curves show a sharp increase in the True Positive Rate (TPR) with a very small increase in the False Positive Rate (FPR), indicating that the model has a high sensitivity (recall) and specificity.\n"
      ],
      "metadata": {
        "id": "soHGsgBUorGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Random Forest**"
      ],
      "metadata": {
        "id": "v0NkbX7FuYYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Function for training and evaluating a model using Stratified K-Fold cross-validation\n",
        "def evaluate_model(model, X, y):\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    accuracy_scores = []\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "    roc_aucs = []\n",
        "\n",
        "    for train_index, test_index in skf.split(X, y):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else y_pred\n",
        "\n",
        "        accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
        "        precisions.append(precision_score(y_test, y_pred))\n",
        "        recalls.append(recall_score(y_test, y_pred))\n",
        "        f1_scores.append(f1_score(y_test, y_pred))\n",
        "        roc_aucs.append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "    return accuracy_scores, precisions, recalls, f1_scores, roc_aucs\n",
        "\n",
        "# Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_metrics = evaluate_model(rf_model, X_scaled, y)\n",
        "print_metrics_summary('Random Forest Accuracy', rf_metrics[0])\n",
        "print_metrics_summary('Random Forest Precision', rf_metrics[1])\n",
        "print_metrics_summary('Random Forest Recall', rf_metrics[2])\n",
        "print_metrics_summary('Random Forest F1 Score', rf_metrics[3])\n",
        "print_metrics_summary('Random Forest ROC AUC', rf_metrics[4])"
      ],
      "metadata": {
        "id": "uErH2VqvurVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CART**"
      ],
      "metadata": {
        "id": "ypszSO5zwFAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CART (Decision Tree)\n",
        "cart_model = DecisionTreeClassifier(random_state=42)\n",
        "cart_metrics = evaluate_model(cart_model, X_scaled, y)\n",
        "print_metrics_summary('CART Accuracy', cart_metrics[0])\n",
        "print_metrics_summary('CART Precision', cart_metrics[1])\n",
        "print_metrics_summary('CART Recall', cart_metrics[2])\n",
        "print_metrics_summary('CART F1 Score', cart_metrics[3])\n",
        "print_metrics_summary('CART ROC AUC', cart_metrics[4])"
      ],
      "metadata": {
        "id": "jBXTJKgYvC2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **C5.0**"
      ],
      "metadata": {
        "id": "jmkXX55iwIZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C5.0 (using Decision Tree with entropy criterion as proxy)\n",
        "c50_model = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
        "c50_metrics = evaluate_model(c50_model, X_scaled, y)\n",
        "print_metrics_summary('C5.0 Accuracy', c50_metrics[0])\n",
        "print_metrics_summary('C5.0 Precision', c50_metrics[1])\n",
        "print_metrics_summary('C5.0 Recall', c50_metrics[2])\n",
        "print_metrics_summary('C5.0 F1 Score', c50_metrics[3])\n",
        "print_metrics_summary('C5.0 ROC AUC', c50_metrics[4])"
      ],
      "metadata": {
        "id": "DH7tBTe0vPEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Naive Bayes**"
      ],
      "metadata": {
        "id": "cnECzk1RwMzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating Naive Bayes\n",
        "nb_model = GaussianNB()\n",
        "nb_metrics = evaluate_model(nb_model, X_scaled, y)\n",
        "print_metrics_summary('Naive Bayes Accuracy', nb_metrics[0])\n",
        "print_metrics_summary('Naive Bayes Precision', nb_metrics[1])\n",
        "print_metrics_summary('Naive Bayes Recall', nb_metrics[2])\n",
        "print_metrics_summary('Naive Bayes F1 Score', nb_metrics[3])\n",
        "print_metrics_summary('Naive Bayes ROC AUC', nb_metrics[4])"
      ],
      "metadata": {
        "id": "bIFKwVoTvRfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ROC Curves for All Models**"
      ],
      "metadata": {
        "id": "mXkofnSRvY9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the ROC curves for all models\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for model, name in zip([rf_model, cart_model, c50_model, nb_model], ['Random Forest', 'CART', 'C5.0', 'Naive Bayes']):\n",
        "    mean_tpr = np.zeros_like(mean_fpr)\n",
        "    for train_index, test_index in skf.split(X_scaled, y):\n",
        "        X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else model.predict(X_test)\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "        mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
        "\n",
        "    mean_tpr /= skf.get_n_splits()\n",
        "    mean_auc = auc(mean_fpr, mean_tpr)\n",
        "    plt.plot(mean_fpr, mean_tpr, lw=2, label=f'{name} (area = {mean_auc:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QUY014P-vTst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Interpretation of the ROC Curve Plot**\n",
        "\n",
        "**Random Forest (AUC = 0.99):** The red line represents the Random Forest classifier, which shows excellent performance with an AUC of 0.99. The curve is very close to the top-left corner, indicating a high TPR and a low FPR across various threshold settings. This suggests that the Random Forest model is highly effective at distinguishing between the malignant and benign tumor cases.\n",
        "\n",
        "**CART (AUC = 0.93):** The blue line represents the CART (Decision Tree) classifier. With an AUC of 0.93, this model also performs well, but not as well as the Random Forest. The curve is slightly further from the top-left corner, indicating slightly lower sensitivity and specificity compared to the Random Forest.\n",
        "\n",
        "**C5.0 (AUC = 0.93):** The purple line represents the C5.0 model (simulated using a Decision Tree with the entropy criterion). It has an identical AUC to the CART model, showing similar performance in terms of discriminative ability. The overlap in curves indicates that both models have comparable effectiveness.\n",
        "\n",
        "**Naive Bayes (AUC = 0.99):** The black line represents the Naive Bayes classifier, which also shows excellent performance with an AUC of 0.99. The curve is close to the top-left corner, similar to the Random Forest, indicating high sensitivity and specificity.\n",
        "\n",
        "**Excellent**\n",
        "\n",
        "The Random Forest and Naive Bayes models both show excellent performance with AUCs of 0.99. Their ROC curves indicate that they have a high capability to correctly classify malignant and benign tumors.\n",
        "\n",
        "**Good**\n",
        "\n",
        "The CART and C5.0 models also perform well with AUCs of 0.93. While not as high as Random Forest and Naive Bayes, they still demonstrate strong discriminative ability.\n",
        "\n",
        "**Consistency**\n",
        "\n",
        "The consistency in high AUC values across models suggests that the features selected (\"area_worst\", \"smoothness_worst\", and \"texture_mean\") are effective in distinguishing between the two classes in the breast cancer dataset.\n",
        "\n",
        "The ROC curve is a valuable tool for comparing the performance of different classifiers. In this case, it highlights that both the Random Forest and Naive Bayes models are particularly effective for the given dataset."
      ],
      "metadata": {
        "id": "sZPpIirixI_j"
      }
    }
  ]
}